{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install once\n",
    "# !pip install -U boto3 sagemaker awscli\n",
    "# restart jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pydev2/lib/python3.10/site-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:10:40] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:10:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=788738;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=355451;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:10:41] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:10:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=924758;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=37688;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# role = get_execution_role()\n",
    "role = 'arn:aws:iam::633205212955:role/service-role/AmazonSageMaker-ExecutionRole-20220923T160810'\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "# image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "# image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.4.0-gpu-py311-cu124-ubuntu22.04-sagemaker'\n",
    "# image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.5.1-gpu-py311-cu124-ubuntu22.04-sagemaker'\n",
    "\n",
    "# instance_type = \"ml.g5.2xlarge\"    # 1 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.12xlarge\"     # 4 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.48xlarge\"    # 8 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.p4d.24xlarge\"   # 8 * A100 (40G/GPU)\n",
    "instance_type = \"ml.p5.48xlarge\"    # 8 * H100 (80G/GPU)\n",
    "# instance_type = \"ml.g6e.48xlarge\"    # 8 * H100 (80G/GPU)\n",
    "# instance_type = \"ml.p3dn.24xlarge\"    # 8 * A10g (24G/GPU)\n",
    "\n",
    "instance_count = 2                  # 1 or Multi-node\n",
    "\n",
    "llamafactory_yaml = 'llama3_full_dpo_z2_1_4'\n",
    "envs = {\n",
    "    # \"DATA_S3_PATH\": f's3://{sagemaker_default_bucket}/qwen2-train-dataset/*',\n",
    "    # 'MODEL_ID_OR_S3_PATH': f's3://llm-artifacts-us-east-1/MTLM-llama-3-8b-instruct/*', \n",
    "    'MODEL_ID_OR_S3_PATH': f's3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/*',\n",
    "    'MODEL_SAVE_PATH_S3': f's3://{sagemaker_default_bucket}/output-model/241201/',\n",
    "    'CONF_YAML_NAME': f'{llamafactory_yaml}.yaml'\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "}\n",
    "\n",
    "smp_estimator = Estimator(role=role,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=f'inst-{instance_count}'+llamafactory_yaml.replace('_','-'),\n",
    "    entry_point=\"estimator_entry.py\",\n",
    "    source_dir='submit_src/',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    environment=envs,\n",
    "    hyperparameters=hypers,\n",
    "    image_uri=image_uri,\n",
    "    max_run=7200,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    enable_remote_debug=True,\n",
    "    disable_output_compression=True,\n",
    ")\n",
    "\n",
    "# smp_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------PROGRESS---------:  2\n",
      "---config---: {'path': 'submit_src/configs/genConf_ds_z1_mbs1_acm8.yaml', 'conf_file_name': 'genConf_ds_z1_mbs1_acm8.yaml', 'params': {'zero_conf': 'ds_z1', 'micro_bs': 1, 'accum_steps': 8}}\n",
      "---JOB NAME---: inst1-z1-mbs1-acm8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:13:20] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#90\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:13:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=407248;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=194507;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:13:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         inst1-z1-mbs1-acm8-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-10-13-13-20-829                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:13:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=973888;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=494531;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         inst1-z1-mbs1-acm8-\u001b[1;36m2024\u001b[0m-12-10-13-13-20-829                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 13:13:30 Starting - Starting the training job...\n",
      "2024-12-10 13:13:44 Pending - Training job waiting for capacity.........\n",
      "2024-12-10 13:15:15 Pending - Preparing the instances for training................................................\n",
      "2024-12-10 13:23:34 Downloading - Downloading input data......\n",
      "2024-12-10 13:24:10 Downloading - Downloading the training image..................\n",
      "2024-12-10 13:27:12 Training - Training image download completed. Training in progress...bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "/opt/conda/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/opt/conda/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "2024-12-10 13:27:53,362 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-12-10 13:27:53,629 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:27:53,640 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-12-10 13:27:53,642 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-12-10 13:27:57,251 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\n",
      "Collecting deepspeed==0.14.0 (from -r requirements.txt (line 1))\n",
      "Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 50.6 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.11.1.1)\n",
      "Collecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.7.1)\n",
      "Collecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400403 sha256=7fc9e50d41b48cc81e446351b11e09c8713536c70207ad3bd1a371072a9ddffc\n",
      "Stored in directory: /root/.cache/pip/wheels/21/93/10/aca4f9f9390297a80a58fb8db0fcdcf1f41499d1afe922a513\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, pynvml, deepspeed\n",
      "Successfully installed deepspeed-0.14.0 hjson-3.1.0 nvidia-ml-py-12.560.30 py-cpuinfo-9.0.0 pynvml-12.0.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2024-12-10 13:28:14,929 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-12-10 13:28:14,929 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-12-10 13:28:15,206 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,467 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,715 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,727 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"estimator_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"estimator_entry.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=estimator_entry.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p5.48xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=estimator_entry\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=192\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\",\"module_name\":\"estimator_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"estimator_entry.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.11 estimator_entry.py\n",
      "2024-12-10 13:28:15,728 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-12-10 13:28:15,728 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Obtaining file:///opt/ml/code/LLaMA-Factory\n",
      "Installing build dependencies: started\n",
      "Installing build dependencies: finished with status 'done'\n",
      "Checking if build backend supports build_editable: started\n",
      "Checking if build backend supports build_editable: finished with status 'done'\n",
      "Getting requirements to build editable: started\n",
      "Getting requirements to build editable: finished with status 'done'\n",
      "Preparing editable metadata (pyproject.toml): started\n",
      "Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting transformers<=4.46.1,>=4.41.2 (from llamafactory==0.9.2.dev0)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets<=3.1.0,>=2.16.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate<=1.0.1,>=0.34.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.2.dev0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.2.dev0)\n",
      "Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.20.4,>=0.19.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<5.0.0,>=4.0.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
      "Collecting sentencepiece (from llamafactory==0.9.2.dev0)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from llamafactory==0.9.2.dev0)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (3.20.3)\n",
      "Collecting uvicorn (from llamafactory==0.9.2.dev0)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.7.1)\n",
      "Collecting fastapi (from llamafactory==0.9.2.dev0)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.2.dev0)\n",
      "Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (3.8.4)\n",
      "Collecting fire (from llamafactory==0.9.2.dev0)\n",
      "Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
      "Collecting av (from llamafactory==0.9.2.dev0)\n",
      "Downloading av-14.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.3.0)\n",
      "Collecting nltk (from llamafactory==0.9.2.dev0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba (from llamafactory==0.9.2.dev0)\n",
      "Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 135.1 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting rouge-chinese (from llamafactory==0.9.2.dev0)\n",
      "Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.66.4)\n",
      "Collecting xxhash (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.5.0)\n",
      "Collecting aiohttp (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.1.5)\n",
      "Collecting orjson~=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (10.3.0)\n",
      "Collecting pydub (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.11.0)\n",
      "Collecting urllib3~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->llamafactory==0.9.2.dev0)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.9.2.dev0) (2.18.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.3)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.2.dev0)\n",
      "Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting termcolor (from fire->llamafactory==0.9.2.dev0)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->llamafactory==0.9.2.dev0) (1.4.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from rouge-chinese->llamafactory==0.9.2.dev0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.7)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.1/18.1 MB 174.8 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 127.4 MB/s eta 0:00:00\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 131.3 MB/s eta 0:00:00\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Downloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Downloading av-14.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.8/33.8 MB 142.3 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 92.3 MB/s eta 0:00:00\n",
      "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 80.9 MB/s eta 0:00:00\n",
      "Downloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 91.8 MB/s eta 0:00:00\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 96.1 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.7/792.7 kB 62.9 MB/s eta 0:00:00\n",
      "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 144.7 MB/s eta 0:00:00\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Building wheels for collected packages: llamafactory, fire, jieba\n",
      "Building editable for llamafactory (pyproject.toml): started\n",
      "Building editable for llamafactory (pyproject.toml): finished with status 'done'\n",
      "Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=23266 sha256=07e88d62914b94569447767fff171354b2c1ad344dbfd4a4cdabcb1d99c47380\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-xx7pgyoa/wheels/6b/38/9f/e1135c70550b8bab6bd9ee2960b7c0a28812185b0b02b9a215\n",
      "Building wheel for fire (setup.py): started\n",
      "Building wheel for fire (setup.py): finished with status 'done'\n",
      "Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=6adaf03b5ae1ebcb135cbb7d15ec0412bec695189d9f3d874da2b5af460ac425\n",
      "Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Building wheel for jieba (setup.py): started\n",
      "Building wheel for jieba (setup.py): finished with status 'done'\n",
      "Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=7ea26f2f853fd8f775e8777d615303228ba4b4c2bbbb3bdfc0b512dc21ddf89d\n",
      "Stored in directory: /root/.cache/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "Successfully built llamafactory fire jieba\n",
      "Installing collected packages: sentencepiece, pydub, jieba, xxhash, websockets, urllib3, tomlkit, termcolor, sniffio, shtab, semantic-version, ruff, rouge-chinese, regex, python-multipart, propcache, orjson, multidict, importlib-resources, h11, frozenlist, ffmpy, docstring-parser, av, aiohappyeyeballs, aiofiles, yarl, uvicorn, nltk, httpcore, fire, anyio, aiosignal, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi, accelerate, transformers, gradio, datasets, trl, peft, llamafactory\n",
      "Attempting uninstall: urllib3\n",
      "Found existing installation: urllib3 1.26.20\n",
      "Uninstalling urllib3-1.26.20:\n",
      "Successfully uninstalled urllib3-1.26.20\n",
      "Attempting uninstall: typer\n",
      "Found existing installation: typer 0.9.4\n",
      "Uninstalling typer-0.9.4:\n",
      "Successfully uninstalled typer-0.9.4\n",
      "Attempting uninstall: huggingface-hub\n",
      "Found existing installation: huggingface_hub 0.23.0\n",
      "Uninstalling huggingface_hub-0.23.0:\n",
      "Successfully uninstalled huggingface_hub-0.23.0\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.30.1\n",
      "Uninstalling accelerate-0.30.1:\n",
      "Successfully uninstalled accelerate-0.30.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.15.1 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.15.1 which is incompatible.\n",
      "Successfully installed accelerate-1.0.1 aiofiles-23.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 anyio-4.7.0 av-14.0.1 datasets-3.1.0 docstring-parser-0.16 fastapi-0.115.6 ffmpy-0.4.0 fire-0.7.0 frozenlist-1.5.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.26.5 importlib-resources-6.4.5 jieba-0.42.1 llamafactory-0.9.2.dev0 multidict-6.1.0 nltk-3.9.1 orjson-3.10.12 peft-0.12.0 propcache-0.2.1 pydub-0.25.1 python-multipart-0.0.19 regex-2024.11.6 rouge-chinese-1.0.3 ruff-0.8.2 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.3 starlette-0.41.3 termcolor-2.5.0 tiktoken-0.8.0 tokenizers-0.20.3 tomlkit-0.12.0 transformers-4.46.1 trl-0.9.6 typer-0.15.1 tyro-0.8.14 urllib3-2.2.3 uvicorn-0.32.1 websockets-12.0 xxhash-3.5.0 yarl-1.18.3\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "Requirement already satisfied: flash-attn in /opt/conda/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from flash-attn) (2.3.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from flash-attn) (23.2)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "sh: 1: wandb: not found\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/.gitignore /tmp/initial-model-path/.cache/huggingface/.gitignore\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/LICENSE.txt.lock /tmp/initial-model-path/.cache/huggingface/download/LICENSE.txt.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/LICENSE.txt.metadata /tmp/initial-model-path/.cache/huggingface/download/LICENSE.txt.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/README.md.metadata /tmp/initial-model-path/.cache/huggingface/download/README.md.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/.gitattributes.lock /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/README.md.lock /tmp/initial-model-path/.cache/huggingface/download/README.md.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/USE_POLICY.md.metadata /tmp/initial-model-path/.cache/huggingface/download/USE_POLICY.md.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/config.json.lock /tmp/initial-model-path/.cache/huggingface/download/config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/USE_POLICY.md.lock /tmp/initial-model-path/.cache/huggingface/download/USE_POLICY.md.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/generation_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00001-of-00002.safetensors.metadata /tmp/initial-model-path/.cache/huggingface/download/model-00001-of-00002.safetensors.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00002-of-00002.safetensors.metadata /tmp/initial-model-path/.cache/huggingface/download/model-00002-of-00002.safetensors.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00002-of-00002.safetensors.lock /tmp/initial-model-path/.cache/huggingface/download/model-00002-of-00002.safetensors.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00001-of-00002.safetensors.lock /tmp/initial-model-path/.cache/huggingface/download/model-00001-of-00002.safetensors.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model.safetensors.index.json.metadata /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.index.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model.safetensors.index.json.lock /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/orig_params.json.lock /tmp/initial-model-path/.cache/huggingface/download/original/orig_params.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/consolidated.00.pth.lock /tmp/initial-model-path/.cache/huggingface/download/original/consolidated.00.pth.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/tokenizer.model.metadata /tmp/initial-model-path/.cache/huggingface/download/original/tokenizer.model.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/orig_params.json.metadata /tmp/initial-model-path/.cache/huggingface/download/original/orig_params.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/special_tokens_map.json.metadata /tmp/initial-model-path/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/params.json.lock /tmp/initial-model-path/.cache/huggingface/download/original/params.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/params.json.metadata /tmp/initial-model-path/.cache/huggingface/download/original/params.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/tokenizer.model.lock /tmp/initial-model-path/.cache/huggingface/download/original/tokenizer.model.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/special_tokens_map.json.lock /tmp/initial-model-path/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/generation_config.json /tmp/initial-model-path/generation_config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.gitattributes /tmp/initial-model-path/.gitattributes\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/tokenizer_config.json /tmp/initial-model-path/tokenizer_config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/LICENSE.txt /tmp/initial-model-path/LICENSE.txt\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/special_tokens_map.json /tmp/initial-model-path/special_tokens_map.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/README.md /tmp/initial-model-path/README.md\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/USE_POLICY.md /tmp/initial-model-path/USE_POLICY.md\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/config.json /tmp/initial-model-path/config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model.safetensors.index.json /tmp/initial-model-path/model.safetensors.index.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/tokenizer.json /tmp/initial-model-path/tokenizer.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model-00002-of-00002.safetensors /tmp/initial-model-path/model-00002-of-00002.safetensors\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/consolidated.00.pth.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete /tmp/initial-model-path/.cache/huggingface/download/original/consolidated.00.pth.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model-00001-of-00002.safetensors /tmp/initial-model-path/model-00001-of-00002.safetensors\n",
      "[2024-12-10 13:29:15,150] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2024-12-10 13:29:17] llamafactory.cli:157 >> Initializing distributed tasks at: algo-1:7777\n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] \n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] *****************************************\n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] *****************************************\n",
      "[2024-12-10 13:29:32,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,337] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,364] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,424] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "FileNotFoundError:\n",
      "[Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "launch()model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "     File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "                                                 run_exp() \n",
      "              File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "                ^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args) \n",
      "                                                                       ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "     File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "return _parse_args(parser, args) \n",
      "                                 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      " ^^^  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "    return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^    ^return _parse_args(parser, args)^\n",
      "^^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "    return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^^^^^    ^outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)^\n",
      "^\n",
      "^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "     File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "                                             ^^^^^^^^^^^^^^^\n",
      "^^    ^with self.open(mode='r', encoding=encoding, errors=errors) as f:^\n",
      "^^^^^^\n",
      "^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "         ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^^^    ^return io.open(self, mode, buffering, encoding, errors, newline)^\n",
      "^^^^^^^^^^^^^^^^^ ^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "      ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "W1210 13:29:34.085000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 265 closing signal SIGTERM\n",
      "W1210 13:29:34.085000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 266 closing signal SIGTERM\n",
      "W1210 13:29:34.086000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 268 closing signal SIGTERM\n",
      "W1210 13:29:34.086000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 269 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 270 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 271 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 272 closing signal SIGTERM\n",
      "E1210 13:29:34.652000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 2 (pid: 267) of binary: /opt/conda/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      "sys.exit(load_entry_point('torch==2.3.0', 'console_scripts', 'torchrun')())\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "return f(*args, **kwargs)\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "run(args)\n",
      "File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "return launch_agent(self._config, self._entrypoint, list(args))\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-12-10_13:29:34\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 267)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "ERROR \"cp /tmp/tuned-model-path/ s3://sagemaker-us-east-1-633205212955/output-model/241201/\": given object /tmp/tuned-model-path/ not found\n",
      "2024-12-10 13:29:36,566 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-12-10 13:29:36,566 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-12-10 13:29:36,567 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2024-12-10 13:29:46 Uploading - Uploading generated training model\n",
      "2024-12-10 13:29:46 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 371\n",
      "Billable seconds: 371\n",
      "---_current_job_name---: inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "/opt/conda/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/opt/conda/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "2024-12-10 13:27:53,362 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-12-10 13:27:53,629 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:27:53,640 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-12-10 13:27:53,642 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-12-10 13:27:57,251 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\n",
      "Collecting deepspeed==0.14.0 (from -r requirements.txt (line 1))\n",
      "Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 50.6 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.11.1.1)\n",
      "Collecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.7.1)\n",
      "Collecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0->-r requirements.txt (line 1))\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400403 sha256=7fc9e50d41b48cc81e446351b11e09c8713536c70207ad3bd1a371072a9ddffc\n",
      "Stored in directory: /root/.cache/pip/wheels/21/93/10/aca4f9f9390297a80a58fb8db0fcdcf1f41499d1afe922a513\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, pynvml, deepspeed\n",
      "Successfully installed deepspeed-0.14.0 hjson-3.1.0 nvidia-ml-py-12.560.30 py-cpuinfo-9.0.0 pynvml-12.0.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2024-12-10 13:28:14,929 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-12-10 13:28:14,929 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-12-10 13:28:15,206 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,467 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,715 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-12-10 13:28:15,727 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"estimator_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"estimator_entry.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=estimator_entry.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p5.48xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=estimator_entry\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=192\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/inst1-z1-mbs1-acm8-2024-12-10-13-13-20-829/source/sourcedir.tar.gz\",\"module_name\":\"estimator_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"estimator_entry.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.11 estimator_entry.py\n",
      "2024-12-10 13:28:15,728 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-12-10 13:28:15,728 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Obtaining file:///opt/ml/code/LLaMA-Factory\n",
      "Installing build dependencies: started\n",
      "Installing build dependencies: finished with status 'done'\n",
      "Checking if build backend supports build_editable: started\n",
      "Checking if build backend supports build_editable: finished with status 'done'\n",
      "Getting requirements to build editable: started\n",
      "Getting requirements to build editable: finished with status 'done'\n",
      "Preparing editable metadata (pyproject.toml): started\n",
      "Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting transformers<=4.46.1,>=4.41.2 (from llamafactory==0.9.2.dev0)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets<=3.1.0,>=2.16.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate<=1.0.1,>=0.34.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.2.dev0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.2.dev0)\n",
      "Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.20.4,>=0.19.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<5.0.0,>=4.0.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
      "Collecting sentencepiece (from llamafactory==0.9.2.dev0)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from llamafactory==0.9.2.dev0)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (3.20.3)\n",
      "Collecting uvicorn (from llamafactory==0.9.2.dev0)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.7.1)\n",
      "Collecting fastapi (from llamafactory==0.9.2.dev0)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.2.dev0)\n",
      "Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (3.8.4)\n",
      "Collecting fire (from llamafactory==0.9.2.dev0)\n",
      "Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
      "Collecting av (from llamafactory==0.9.2.dev0)\n",
      "Downloading av-14.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.2.dev0)\n",
      "Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.9.2.dev0) (2.3.0)\n",
      "Collecting nltk (from llamafactory==0.9.2.dev0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba (from llamafactory==0.9.2.dev0)\n",
      "Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 135.1 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting rouge-chinese (from llamafactory==0.9.2.dev0)\n",
      "Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.66.4)\n",
      "Collecting xxhash (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.5.0)\n",
      "Collecting aiohttp (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.1.5)\n",
      "Collecting orjson~=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (10.3.0)\n",
      "Collecting pydub (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.11/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.11.0)\n",
      "Collecting urllib3~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->llamafactory==0.9.2.dev0)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.9.2.dev0) (2.18.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.3)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.2.dev0)\n",
      "Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting termcolor (from fire->llamafactory==0.9.2.dev0)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->llamafactory==0.9.2.dev0) (1.4.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from rouge-chinese->llamafactory==0.9.2.dev0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.7)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.1/18.1 MB 174.8 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 127.4 MB/s eta 0:00:00\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 131.3 MB/s eta 0:00:00\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Downloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Downloading av-14.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.8/33.8 MB 142.3 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 92.3 MB/s eta 0:00:00\n",
      "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 80.9 MB/s eta 0:00:00\n",
      "Downloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 91.8 MB/s eta 0:00:00\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 96.1 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.7/792.7 kB 62.9 MB/s eta 0:00:00\n",
      "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 144.7 MB/s eta 0:00:00\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Building wheels for collected packages: llamafactory, fire, jieba\n",
      "Building editable for llamafactory (pyproject.toml): started\n",
      "Building editable for llamafactory (pyproject.toml): finished with status 'done'\n",
      "Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=23266 sha256=07e88d62914b94569447767fff171354b2c1ad344dbfd4a4cdabcb1d99c47380\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-xx7pgyoa/wheels/6b/38/9f/e1135c70550b8bab6bd9ee2960b7c0a28812185b0b02b9a215\n",
      "Building wheel for fire (setup.py): started\n",
      "Building wheel for fire (setup.py): finished with status 'done'\n",
      "Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=6adaf03b5ae1ebcb135cbb7d15ec0412bec695189d9f3d874da2b5af460ac425\n",
      "Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Building wheel for jieba (setup.py): started\n",
      "Building wheel for jieba (setup.py): finished with status 'done'\n",
      "Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=7ea26f2f853fd8f775e8777d615303228ba4b4c2bbbb3bdfc0b512dc21ddf89d\n",
      "Stored in directory: /root/.cache/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "Successfully built llamafactory fire jieba\n",
      "Installing collected packages: sentencepiece, pydub, jieba, xxhash, websockets, urllib3, tomlkit, termcolor, sniffio, shtab, semantic-version, ruff, rouge-chinese, regex, python-multipart, propcache, orjson, multidict, importlib-resources, h11, frozenlist, ffmpy, docstring-parser, av, aiohappyeyeballs, aiofiles, yarl, uvicorn, nltk, httpcore, fire, anyio, aiosignal, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi, accelerate, transformers, gradio, datasets, trl, peft, llamafactory\n",
      "Attempting uninstall: urllib3\n",
      "Found existing installation: urllib3 1.26.20\n",
      "Uninstalling urllib3-1.26.20:\n",
      "Successfully uninstalled urllib3-1.26.20\n",
      "Attempting uninstall: typer\n",
      "Found existing installation: typer 0.9.4\n",
      "Uninstalling typer-0.9.4:\n",
      "Successfully uninstalled typer-0.9.4\n",
      "Attempting uninstall: huggingface-hub\n",
      "Found existing installation: huggingface_hub 0.23.0\n",
      "Uninstalling huggingface_hub-0.23.0:\n",
      "Successfully uninstalled huggingface_hub-0.23.0\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.30.1\n",
      "Uninstalling accelerate-0.30.1:\n",
      "Successfully uninstalled accelerate-0.30.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.15.1 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.15.1 which is incompatible.\n",
      "Successfully installed accelerate-1.0.1 aiofiles-23.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 anyio-4.7.0 av-14.0.1 datasets-3.1.0 docstring-parser-0.16 fastapi-0.115.6 ffmpy-0.4.0 fire-0.7.0 frozenlist-1.5.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.26.5 importlib-resources-6.4.5 jieba-0.42.1 llamafactory-0.9.2.dev0 multidict-6.1.0 nltk-3.9.1 orjson-3.10.12 peft-0.12.0 propcache-0.2.1 pydub-0.25.1 python-multipart-0.0.19 regex-2024.11.6 rouge-chinese-1.0.3 ruff-0.8.2 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.3 starlette-0.41.3 termcolor-2.5.0 tiktoken-0.8.0 tokenizers-0.20.3 tomlkit-0.12.0 transformers-4.46.1 trl-0.9.6 typer-0.15.1 tyro-0.8.14 urllib3-2.2.3 uvicorn-0.32.1 websockets-12.0 xxhash-3.5.0 yarl-1.18.3\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "Requirement already satisfied: flash-attn in /opt/conda/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from flash-attn) (2.3.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from flash-attn) (23.2)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "sh: 1: wandb: not found\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/.gitignore /tmp/initial-model-path/.cache/huggingface/.gitignore\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/LICENSE.txt.lock /tmp/initial-model-path/.cache/huggingface/download/LICENSE.txt.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/LICENSE.txt.metadata /tmp/initial-model-path/.cache/huggingface/download/LICENSE.txt.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/README.md.metadata /tmp/initial-model-path/.cache/huggingface/download/README.md.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/.gitattributes.lock /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/README.md.lock /tmp/initial-model-path/.cache/huggingface/download/README.md.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/USE_POLICY.md.metadata /tmp/initial-model-path/.cache/huggingface/download/USE_POLICY.md.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/config.json.lock /tmp/initial-model-path/.cache/huggingface/download/config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/USE_POLICY.md.lock /tmp/initial-model-path/.cache/huggingface/download/USE_POLICY.md.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/generation_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00001-of-00002.safetensors.metadata /tmp/initial-model-path/.cache/huggingface/download/model-00001-of-00002.safetensors.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00002-of-00002.safetensors.metadata /tmp/initial-model-path/.cache/huggingface/download/model-00002-of-00002.safetensors.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00002-of-00002.safetensors.lock /tmp/initial-model-path/.cache/huggingface/download/model-00002-of-00002.safetensors.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model-00001-of-00002.safetensors.lock /tmp/initial-model-path/.cache/huggingface/download/model-00001-of-00002.safetensors.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model.safetensors.index.json.metadata /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.index.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/model.safetensors.index.json.lock /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/orig_params.json.lock /tmp/initial-model-path/.cache/huggingface/download/original/orig_params.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/consolidated.00.pth.lock /tmp/initial-model-path/.cache/huggingface/download/original/consolidated.00.pth.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/tokenizer.model.metadata /tmp/initial-model-path/.cache/huggingface/download/original/tokenizer.model.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/orig_params.json.metadata /tmp/initial-model-path/.cache/huggingface/download/original/orig_params.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/special_tokens_map.json.metadata /tmp/initial-model-path/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/params.json.lock /tmp/initial-model-path/.cache/huggingface/download/original/params.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/params.json.metadata /tmp/initial-model-path/.cache/huggingface/download/original/params.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/tokenizer.model.lock /tmp/initial-model-path/.cache/huggingface/download/original/tokenizer.model.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/special_tokens_map.json.lock /tmp/initial-model-path/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/generation_config.json /tmp/initial-model-path/generation_config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.gitattributes /tmp/initial-model-path/.gitattributes\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/tokenizer_config.json /tmp/initial-model-path/tokenizer_config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/LICENSE.txt /tmp/initial-model-path/LICENSE.txt\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/special_tokens_map.json /tmp/initial-model-path/special_tokens_map.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/README.md /tmp/initial-model-path/README.md\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/USE_POLICY.md /tmp/initial-model-path/USE_POLICY.md\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/config.json /tmp/initial-model-path/config.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model.safetensors.index.json /tmp/initial-model-path/model.safetensors.index.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/tokenizer.json /tmp/initial-model-path/tokenizer.json\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model-00002-of-00002.safetensors /tmp/initial-model-path/model-00002-of-00002.safetensors\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/consolidated.00.pth.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete /tmp/initial-model-path/.cache/huggingface/download/original/consolidated.00.pth.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete\n",
      "cp s3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/model-00001-of-00002.safetensors /tmp/initial-model-path/model-00001-of-00002.safetensors\n",
      "[2024-12-10 13:29:15,150] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2024-12-10 13:29:17] llamafactory.cli:157 >> Initializing distributed tasks at: algo-1:7777\n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] \n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] *****************************************\n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1210 13:29:19.070000 140387575871296 torch/distributed/run.py:757] *****************************************\n",
      "[2024-12-10 13:29:32,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,337] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,364] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-12-10 13:29:32,424] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^\n",
      "^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "return _parse_args(parser, args)\n",
      "^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^\n",
      "FileNotFoundError:\n",
      "[Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "launch()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "launch()model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "     File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "                                                 run_exp() \n",
      "              File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 45, in run_exp\n",
      "                ^^^^\n",
      "^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args) \n",
      "                                                                       ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "     File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 161, in get_train_args\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "return _parse_args(parser, args) \n",
      "                                 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      " ^^^  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 147, in _parse_train_args\n",
      "    return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^    ^return _parse_args(parser, args)^\n",
      "^^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ml/code/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 60, in _parse_args\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "    return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
      "^^^^^^^^^    ^outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)^\n",
      "^\n",
      "^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "     File \"/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py\", line 436, in parse_yaml_file\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)\n",
      "                                             ^^^^^^^^^^^^^^^\n",
      "^^    ^with self.open(mode='r', encoding=encoding, errors=errors) as f:^\n",
      "^^^^^^\n",
      "^^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
      "         ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "  File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^\n",
      "^^^^^^^^^    ^return io.open(self, mode, buffering, encoding, errors, newline)^\n",
      "^^^^^^^^^^^^^^^^^ ^\n",
      "File \"/opt/conda/lib/python3.11/pathlib.py\", line 1044, in open\n",
      "      ^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/code/configs/genConf_ds_z1_mbs1_acm8.yaml.yaml'\n",
      "W1210 13:29:34.085000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 265 closing signal SIGTERM\n",
      "W1210 13:29:34.085000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 266 closing signal SIGTERM\n",
      "W1210 13:29:34.086000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 268 closing signal SIGTERM\n",
      "W1210 13:29:34.086000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 269 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 270 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 271 closing signal SIGTERM\n",
      "W1210 13:29:34.087000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 272 closing signal SIGTERM\n",
      "E1210 13:29:34.652000 140387575871296 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 2 (pid: 267) of binary: /opt/conda/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      "sys.exit(load_entry_point('torch==2.3.0', 'console_scripts', 'torchrun')())\n",
      "^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "return f(*args, **kwargs)\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "run(args)\n",
      "File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "return launch_agent(self._config, self._entrypoint, list(args))\n",
      "^^^^^^^^^^^\n",
      "^^^^^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/opt/ml/code/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-12-10_13:29:34\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 267)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "ERROR \"cp /tmp/tuned-model-path/ s3://sagemaker-us-east-1-633205212955/output-model/241201/\": given object /tmp/tuned-model-path/ not found\n",
      "2024-12-10 13:29:36,566 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-12-10 13:29:36,566 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-12-10 13:29:36,567 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "---------PROGRESS---------:  4\n",
      "---config---: {'path': 'submit_src/configs/genConf_ds_z1_mbs2_acm4.yaml', 'conf_file_name': 'genConf_ds_z1_mbs2_acm4.yaml', 'params': {'zero_conf': 'ds_z1', 'micro_bs': 2, 'accum_steps': 4}}\n",
      "---JOB NAME---: inst1-z1-mbs2-acm4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:30:23] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#90\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:30:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=443511;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=171628;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 13:30:33] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         inst1-z1-mbs2-acm4-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-10-13-30-23-262                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 13:30:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=710539;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=478241;file:///opt/conda/envs/pydev2/lib/python3.10/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         inst1-z1-mbs2-acm4-\u001b[1;36m2024\u001b[0m-12-10-13-30-23-262                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 13:30:36 Starting - Starting the training job\n",
      "2024-12-10 13:30:36 Pending - Training job waiting for capacity.........\n",
      "2024-12-10 13:32:00 Pending - Preparing the instances for training................................................\n",
      "2024-12-10 13:39:58 Downloading - Downloading the training image.............."
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "\n",
    "import boto3\n",
    "logs_client = boto3.client('logs')\n",
    "log_group_name = \"/aws/sagemaker/TrainingJobs\"\n",
    "\n",
    "from config_gens import *\n",
    "\n",
    "# 变量定义\n",
    "variables = {\n",
    "    'zero_conf': [\"ds_z1\",\"ds_z2\",\"ds_z3\",\"ds_z2_offload\",\"ds_z3_offload\"],\n",
    "    'micro_bs': [1,2,4,8],\n",
    "    'accum_steps': [2,4,8]\n",
    "}\n",
    "\n",
    "# 初始化生成器\n",
    "generator = ConfigGenerator('submit_src/llama3_full_dpo_template.yaml')\n",
    "\n",
    "# 生成配置文件\n",
    "configs = generator.generate_configs(\n",
    "    variables=variables,\n",
    "    output_dir='submit_src/configs/',\n",
    "    filename_template=\"genConf_{zero_conf}_mbs{micro_bs}_acm{accum_steps}.yaml\"\n",
    ")\n",
    "\n",
    "configs_dict = {i: value for i, value in enumerate(configs)}\n",
    "\n",
    "\n",
    "GLB_BS = 64\n",
    "\n",
    "for inst in [1]:\n",
    "    for config_i in configs_dict.keys():\n",
    "        \n",
    "        config = configs_dict[config_i]\n",
    "\n",
    "        gen_bs = config['params']['micro_bs']\n",
    "        gen_accum = config['params']['accum_steps']\n",
    "\n",
    "        if inst*8*gen_bs*gen_accum != GLB_BS:\n",
    "            continue\n",
    "        \n",
    "        # skip list\n",
    "        skip_list = [## z1 and big bs\n",
    "                     'z1_mbs8', \n",
    "                     'z2_mbs1', \n",
    "                     ## test done\n",
    "                     'z3_mbs1',\n",
    "                     ## will be slower\n",
    "                     'z2_offload_mbs1',\n",
    "                     'z3_offload_mbs1',\n",
    "                     ## oom\n",
    "\n",
    "                     ## done\n",
    "                     'z1_mbs1_acm8'\n",
    "                     ]\n",
    "        # for sl in skip_list:\n",
    "        #     if sl not in config['conf_file_name']:\n",
    "        #         continue\n",
    "\n",
    "        if any(sl in config['conf_file_name'] for sl in skip_list):\n",
    "            continue\n",
    "\n",
    "        print('---------PROGRESS---------: ', config_i)\n",
    "        print('---config---:', config)\n",
    "        namestr = f'inst{inst}-' + config['conf_file_name'].replace('genConf_ds_','').replace('.yaml','').replace('_','-')\n",
    "        print('---JOB NAME---:', namestr)\n",
    "\n",
    "        envs = {\n",
    "            # \"DATA_S3_PATH\": f's3://{sagemaker_default_bucket}/qwen2-train-dataset/*',\n",
    "            # 'MODEL_ID_OR_S3_PATH': f's3://llm-artifacts-us-east-1/MTLM-llama-3-8b-instruct/*', \n",
    "            'MODEL_ID_OR_S3_PATH': f's3://llm-artifacts-us-east-1/Llama-3.2-3B-Instruct/*',\n",
    "            'MODEL_SAVE_PATH_S3': f's3://{sagemaker_default_bucket}/output-model/241201/',\n",
    "            'CONF_YAML_NAME': f'''configs/{config['conf_file_name']}.yaml'''\n",
    "        }\n",
    "\n",
    "        instance_type = \"ml.p5.48xlarge\"\n",
    "        \n",
    "        smp_estimator = Estimator(role=role,\n",
    "            sagemaker_session=sess,\n",
    "            base_job_name=namestr,\n",
    "            entry_point=\"estimator_entry.py\",\n",
    "            source_dir='submit_src/',\n",
    "            instance_type=instance_type,\n",
    "            instance_count=inst,\n",
    "            environment=envs,\n",
    "            hyperparameters={},\n",
    "            image_uri=image_uri,\n",
    "            max_run=7200,\n",
    "            keep_alive_period_in_seconds=1800,\n",
    "            enable_remote_debug=True,\n",
    "            disable_output_compression=True,\n",
    "        )\n",
    "\n",
    "        # smp_estimator.fit()\n",
    "\n",
    "        try:\n",
    "            smp_estimator.fit()\n",
    "        except Exception as e:\n",
    "            print('---training job breaks---')\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # break\n",
    "\n",
    "        training_job_name = smp_estimator.latest_training_job.job_name\n",
    "        print('---_current_job_name---:', training_job_name)\n",
    "        \n",
    "        response = logs_client.describe_log_streams(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamNamePrefix=training_job_name\n",
    "        )\n",
    "\n",
    "        with open(f'smest_logs/log-{namestr}.logs', 'w') as f:\n",
    "            for stream in response['logStreams']:\n",
    "                log_stream_name = stream['logStreamName']\n",
    "                logs = logs_client.get_log_events(\n",
    "                    logGroupName=log_group_name,\n",
    "                    logStreamName=log_stream_name\n",
    "                )\n",
    "                \n",
    "                for event in logs['events']:\n",
    "                    print(event['message'])\n",
    "                    f.write(event['message'] + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_full_dpo_z2_1_8, inst2, 4.33s/it\n",
    "\n",
    "\n",
    "llama3_full_dpo_z2_1_4, inst2, 2.10s/it\n",
    "llama3_full_dpo_z2_1_8, inst1, 3.70s/it\n",
    "\n",
    "llama3_full_dpo_z1_1_4, inst2, queue1\n",
    "llama3_full_dpo_z1_1_8, inst1, queue2\n",
    "\n",
    "\n",
    "llama3_full_dpo_z3_2_2, inst2, queue3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
